{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import thinkdsp\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File data/dennis_walking_long.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3ddaafac8d71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdata_file_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'data/{}_{}_long.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mactivity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0macts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rlouie/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rlouie/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rlouie/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rlouie/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rlouie/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3163)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5779)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File data/dennis_walking_long.csv does not exist"
     ]
    }
   ],
   "source": [
    "#Ryan's fancy way of inputting data in an easier way\n",
    "data_dict = {'walking':{},'jogging':{},'upstairs':{},'downstairs':{}}\n",
    "names = ['meg','ryan','dennis']\n",
    "acts = ['walking', 'jogging', 'upstairs', 'downstairs']\n",
    "for name in names:\n",
    "    data_file_names = ['data/{}_{}_long.csv'.format(name, activity) for activity in acts]\n",
    "    for i,file in enumerate(data_file_names):\n",
    "        df = pd.read_csv(file)\n",
    "        data_dict[acts[i]][name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = {}\n",
    "\n",
    "for i, (g, gdf) in enumerate(df.groupby('personID')):\n",
    "    db[g] = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON 33\n",
      "   personID activity            time         x          y         z\n",
      "0        33  Jogging  49105962326000 -0.694638  12.680544  0.503953\n",
      "1        33  Jogging  49106062271000  5.012288  11.264028  0.953424\n",
      "2        33  Jogging  49106112167000  4.903325  10.882658 -0.081722\n",
      "3        33  Jogging  49106222305000 -0.612916  18.496431  3.023717\n",
      "4        33  Jogging  49106332290000 -1.184970  12.108489  7.205164\n"
     ]
    }
   ],
   "source": [
    "print \"PERSON {}\".format(33)\n",
    "print db[33].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dominant_frequency import find_dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for personID, df in db.iteritems():\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(vals, framerate, dom_freq_method=\"autocorr\"):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    vals: array-like, shape (n_samples,)\n",
    "        evenly spaced in time values, (i.e. magnitude of the accleration vector)\n",
    "        \n",
    "    dom_freq_method: string, default, \"autocorr\"\n",
    "        available strings:\n",
    "        \"autocorr\" - calculated using autocorrelation\n",
    "        \"spectrum\" - calculated using peaks method of frequency spectrum\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obs: array-like, shape (n_windows, n_features)\n",
    "        current features: amp_dist, dom_freq\n",
    "    \"\"\"\n",
    "    # dictionary of arrays, where the arrays will be a list of amplitudes for each activity\n",
    "    amp_dist = defaultdict(list)\n",
    "    domfreq_dist = defaultdict(list)\n",
    "    domfreq_dist2 = defaultdict(list)\n",
    "\n",
    "    # Groupby the 4 Activites\n",
    "    for g, gdb in db.groupby('activity'):\n",
    "        # Calculate Waves\n",
    "        # FIX: framerate is probably not calculated correctly\n",
    "        framerate = 100000\n",
    "        zwave = thinkdsp.Wave(vals, framerate=framerate)\n",
    "\n",
    "        zwave\n",
    "        start0 = 0\n",
    "        window_size = 0.0003\n",
    "        step_size = window_size / 2\n",
    "        seg_nums = 120\n",
    "\n",
    "        for i in range(seg_nums):\n",
    "            zseg = zwave.segment(start=start0+i*step_size, duration=window_size)\n",
    "\n",
    "            # Unbiasing the Wave to make Spectrum amplitude at frequency 0, equal to 0\n",
    "            zseg.unbias()\n",
    "\n",
    "            # Convert to Frequency Domain\n",
    "            spectrum = zseg.make_spectrum()\n",
    "\n",
    "            # Extract Peaks\n",
    "            # peak[0] the highest peak\n",
    "            peaks = spectrum.peaks()\n",
    "\n",
    "            # appending the amplitude of the highest peak to our dictionary of amplitudes for each activity\n",
    "\n",
    "            # peak[0][0] is the amplitude of the highest peak\n",
    "            # peak[0][1] is the frequency of the highest peak\n",
    "            amp_dist[g].append(peaks[0][0])\n",
    "            domfreq_dist2[g].append(peaks[0][1])\n",
    "\n",
    "            # dominant frequency\n",
    "            domfreq_dist[g].append(find_dominant(zseg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a check, to see the mean and standard deviation that characterize the distribution of peak amplitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking\n",
      "mean amp:  37.0431545312\n",
      "std amp:  8.82675878638\n",
      "mean domfreq:  43497.2527473\n",
      "std domfreq:  42404.6579067\n",
      "mean domfreq2:  22833.3333333\n",
      "std domfreq2:  11663.8885581\n",
      "Jogging\n",
      "mean amp:  51.0915663118\n",
      "std amp:  11.5388338412\n",
      "mean domfreq:  17879.3590669\n",
      "std domfreq:  18868.2820629\n",
      "mean domfreq2:  35638.8888889\n",
      "std domfreq2:  13620.489966\n",
      "Downstairs\n",
      "mean amp:  23.9064234624\n",
      "std amp:  8.66581555018\n",
      "mean domfreq:  24436.7276242\n",
      "std domfreq:  31301.9946498\n",
      "mean domfreq2:  13583.3333333\n",
      "std domfreq2:  10248.4190289\n",
      "Upstairs\n",
      "mean amp:  25.5883485274\n",
      "std amp:  6.18314819799\n",
      "mean domfreq:  18282.4259074\n",
      "std domfreq:  26018.7714273\n",
      "mean domfreq2:  15111.1111111\n",
      "std domfreq2:  5707.91090624\n"
     ]
    }
   ],
   "source": [
    "for (activity, amps), (activity, domfreqs), (activity, domfreqs2) in zip(amp_dist.iteritems(), domfreq_dist.iteritems(), domfreq_dist2.iteritems()):\n",
    "    print activity\n",
    "    print \"mean amp: \",np.array(amps).mean()\n",
    "    print \"std amp: \",np.array(amps).std()\n",
    "    print \"mean domfreq: \", np.array(domfreqs).mean()\n",
    "    print \"std domfreq: \", np.array(domfreqs).std()\n",
    "    print \"mean domfreq2: \", np.array(domfreqs2).mean()\n",
    "    print \"std domfreq2: \", np.array(domfreqs2).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# dict of gaussian hidden markov models for each activity\n",
    "ghmm = {}\n",
    "\n",
    "# dict of train data for each activity\n",
    "X_train = {}\n",
    "\n",
    "# dict of test data for each activity\n",
    "X_test = {}\n",
    "\n",
    "# number of hidden states\n",
    "n_components = 3\n",
    "\n",
    "# number of samples in the training set\n",
    "train_size = 60\n",
    "\n",
    "# Train a separate GHMM for each activity\n",
    "for activity, amps in amp_dist.iteritems():\n",
    "    # Create GHMM\n",
    "    ghmm[activity] = GaussianHMM(n_components, covariance_type=\"diag\", n_iter=1000)\n",
    "    \n",
    "    # Split into Train and Test Data (No Random Shuffling Now)\n",
    "    # If we wanted to add more features:\n",
    "    # X_train[activity] = np.column_stack([amp_dist[activity][:train_size], next_feature[activity][:train_size])\n",
    "    \n",
    "    features = (amp_dist, domfreq_dist2)\n",
    "\n",
    "    X_train[activity] = np.column_stack([feature[activity][:train_size] for feature in features])\n",
    "    X_test[activity] = np.column_stack([feature[activity][train_size:] for feature in features])\n",
    "    \n",
    "    # Fit on Training Data\n",
    "    # Confused about .fit([X])\n",
    "    ghmm[activity].fit([X_train[activity]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual activity: Walking\n",
      "0\n",
      "predicted activity: Jogging\n",
      "logprobs: \n",
      "{'Walking': -908.01678777656798, 'Downstairs': -2835.7986967145257, 'Jogging': -907.94465196170586, 'Upstairs': -1349.7659876207963}\n",
      "\n",
      "\n",
      "actual activity: Downstairs\n",
      "3\n",
      "predicted activity: Downstairs\n",
      "logprobs: \n",
      "{'Walking': -1020.1666103737786, 'Downstairs': -776.81482303212636, 'Jogging': -1024.372651773861, 'Upstairs': -1190.9363556798253}\n",
      "\n",
      "\n",
      "actual activity: Jogging\n",
      "0\n",
      "predicted activity: Jogging\n",
      "logprobs: \n",
      "{'Walking': -1058.3666544800637, 'Downstairs': -12461.628264153218, 'Jogging': -894.18084333649404, 'Upstairs': -4610.4205010823407}\n",
      "\n",
      "\n",
      "actual activity: Upstairs\n",
      "2\n",
      "predicted activity: Upstairs\n",
      "logprobs: \n",
      "{'Walking': -1104.1912395135478, 'Downstairs': -1497.17352096471, 'Jogging': -972.90500722202, 'Upstairs': -394.13945927885669}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities = [\"Jogging\", \"Walking\", \"Upstairs\", \"Downstairs\"]\n",
    "\n",
    "# For each Test Set\n",
    "for activity, X in X_test.iteritems():\n",
    "    print \"actual activity: \" + activity\n",
    "    \n",
    "    # logprobs for each activity_model\n",
    "    # the log-likelihood that the given sequence of observations looks like things this model could produce\n",
    "    logprobs = {}\n",
    "\n",
    "    # Try Out the 4 models\n",
    "    for model_activity, model in ghmm.iteritems():\n",
    "        # model.score returns log likelihood of the observation\n",
    "        logprobs[model_activity] = model.score(X)\n",
    "    \n",
    "\n",
    "    # Which ever has the highest probability will be the model\n",
    "    max_idx = np.argmax(np.array([logprobs[activity] for activity in activities]))\n",
    "    print max_idx\n",
    "    pred_activity = activities[max_idx]\n",
    "    print \"predicted activity: \" + pred_activity\n",
    "    \n",
    "    print \"logprobs: \"\n",
    "    print logprobs\n",
    "    \n",
    "    print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```n_components``` makes a difference, for classifying between Upstairs and Downstairs.  2 and 4 seem to misclassify, 3 and 5 classify accurately.  \n",
    "- note that the dictionary keys will not be in the same order as the ```activities``` list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODOS:\n",
    "\n",
    "- phone accelerometer\n",
    "    - collect our own dataset\n",
    "- timeseries spacing\n",
    "    - can interpolate given the timestamps\n",
    "- feature visualization / extraction\n",
    "    - frequency bins\n",
    "    - n most dominant frequencies\n",
    "- machine learning pipeline building\n",
    "    - Investigating Assumptions of Training / Evaluation Procedure\n",
    "    - Visualizations of labels versus probability classifications\n",
    "    - Randomized Train Test Split\n",
    "    - Cross Validation (how to determine what's the best ```n_components```\n",
    "    \n",
    "##DONE:\n",
    "\n",
    "- phone accelerometer\n",
    "    - install app\n",
    "- feature visualization / extraction\n",
    "    - timeseries\n",
    "    - frequency spectrum\n",
    "    - power spectrum\n",
    "    - phase\n",
    "    - dominant frequency by autocorrelation\n",
    "- machine learning pipeline building\n",
    "    - Hidden Markov Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
